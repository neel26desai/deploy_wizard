{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from LangChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from talk_openai import MyOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "from utils import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neel/Developer/deploy_wizard/talk_openai.py:11: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  self.llm = ChatOpenAI(model=model, temperature=0)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "# Create a ChatOpenAI object \n",
    "inference_script_generator_llm = MyOpenAI(model=MODEL_NAME)\n",
    "inference_script_validator_llm = MyOpenAI(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path():\n",
    "    \"\"\"Prompt the user for the training Python file path.\"\"\"\n",
    "    file_path = input(\"Enter the path to the training Python file: \").strip()\n",
    "    return file_path\n",
    "\n",
    "def validate_file(file_path):\n",
    "    \"\"\"Validate if the file exists and is a Python script.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(\"File not found. Please provide a valid path.\")\n",
    "    \n",
    "    if not file_path.endswith('.py'):\n",
    "        raise ValueError(\"Unsupported file format. Only Python (.py) files are allowed.\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_inputs():\n",
    "    \"\"\"Ask the user to provide model details and data shape.\"\"\"\n",
    "    print(\"To determine the input shape of a single record, you can do the following:\")\n",
    "    print(\"1. If you have a Pandas DataFrame, use df.iloc[0].shape to get the shape of a single row.\")\n",
    "    print(\"2. If using NumPy, call data[0].shape on your dataset.\")\n",
    "    print(\"3. If your data is in a list format, check the length of the first element using len(data[0]).\")\n",
    "    \n",
    "    model_type = input(\"Enter the model type (Binary Classification, Multiclass Classification, Regression): \").strip()\n",
    "    data_shape = input(\"Enter the expected shape of a single row of input data (e.g., (1, 10) for 10 features): \").strip()\n",
    "    return model_type, data_shape\n",
    "\n",
    "\n",
    "\n",
    "# model_type, data_shape = get_user_inputs()\n",
    "model_type = 'Multiclass Classification'\n",
    "data_shape = '(, 4)'\n",
    "# input_file = get_file_path()\n",
    "input_file = '/Users/neel/Developer/deploy_wizard/iris_model_training/training.py'\n",
    "training_script = file_reader(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inference_script_with_llm(training_script, model_type, data_shape, feedback=None):\n",
    "    \"\"\"Use an LLM to generate an inference script based on the provided training script and details.\"\"\"\n",
    "    if feedback is None:        \n",
    "        #if feedback is None, then we are generating the initial prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python developer.\"},\n",
    "            {\"role\": \"user\", \"content\": f'''\n",
    "            You are an expert Python developer. Your task is to generate an inference script that exposes a trained machine learning model as an API.\n",
    "\n",
    "            ### Role: \n",
    "            You are to assist in writing an inference script for an AI agent that will deploy a trained model.\n",
    "\n",
    "            ### Context:\n",
    "            - The training script provided is below:\n",
    "            \"\"\"\n",
    "            {training_script}\n",
    "            \"\"\"\n",
    "            - The model type is {model_type}.\n",
    "            - The expected input shape for predictions is {data_shape}.\n",
    "\n",
    "            ### Action:\n",
    "            Write a Python script that:\n",
    "            - Loads the trained model.\n",
    "            - Accepts input data via an API.\n",
    "            - Preprocesses the input.\n",
    "            - Makes predictions using the model.\n",
    "            - Returns the prediction as JSON.\n",
    "            - Uses FastAPI for API exposure.\n",
    "\n",
    "            ### Expected Output Format:\n",
    "            A complete, standalone Python script that follows best practices and is ready for execution.\n",
    "            '''}\n",
    "            ]\n",
    "    else:\n",
    "        #feedback is present so we treat it as a continuation of the conversation\n",
    "        messages= feedback\n",
    "    response = inference_script_generator_llm.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inference_script_with_llm(inference_script):\n",
    "    \"\"\"Use an LLM to evaluate the generated inference script and provide feedback.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a code reviewer and expert in Python API development.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        You are reviewing an inference script generated for exposing a trained ML model as an API.\n",
    "\n",
    "        ### Task:\n",
    "        - Analyze the script for correctness, best practices, and completeness.\n",
    "        - Identify any missing components or improvements.\n",
    "        - If the script is perfect, respond with 'No changes needed.'\n",
    "        - If changes are required, specify what needs to be improved.\n",
    "\n",
    "        ### Script to review:\n",
    "        ```\n",
    "        {inference_script}\n",
    "        ```\n",
    "\n",
    "        ### Expected Output:\n",
    "        - Either 'No changes needed.' OR a detailed improvement plan.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    \n",
    "    return inference_script_validator_llm.invoke(messages)\n",
    "\n",
    "def actor_critic_inference_script(training_script, model_type, data_shape):\n",
    "    \"\"\"\n",
    "    Runs the actor-critic loop:\n",
    "    - Actor generates an inference script.\n",
    "    - Critic evaluates the script.\n",
    "    - If the script is sufficient, exit early.\n",
    "    - Otherwise, Actor refines the script based on feedback.\n",
    "    - Max iterations: 2\n",
    "    \"\"\"\n",
    "    feedback = None  # No feedback initially\n",
    "\n",
    "    for _ in range(1):  # Max iterations: 1\n",
    "        # Step 1: Actor generates inference script\n",
    "        inference_script = generate_inference_script_with_llm(training_script, model_type, data_shape, feedback)\n",
    "\n",
    "        # Step 2: Critic evaluates the script\n",
    "        critic_feedback = evaluate_inference_script_with_llm(inference_script)\n",
    "\n",
    "        if \"no changes needed\" in critic_feedback.lower():\n",
    "            print(\"âœ… Inference script is satisfactory. Exiting early.\")\n",
    "            return inference_script  # Early exit if script is sufficient\n",
    "\n",
    "        # Step 3: Actor refines script using critic's feedback\n",
    "        feedback = [\n",
    "            {\"role\": \"assistant\", \"content\": inference_script},\n",
    "            {\"role\": \"user\", \"content\": f\"Revise based on this feedback:\\n{critic_feedback}\"}\n",
    "        ]\n",
    "\n",
    "    print(\"ðŸ”„ Max iterations reached. Returning final script.\")\n",
    "    return inference_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Max iterations reached. Returning final script.\n"
     ]
    }
   ],
   "source": [
    "raw_inference_script = actor_critic_inference_script(training_script, model_type, data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a complete Python script that uses FastAPI to expose the trained RandomForestClassifier model as an API. This script will load the model, accept input data, preprocess it, make predictions, and return the results in JSON format.\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic import BaseModel\n",
      "import joblib\n",
      "import numpy as np\n",
      "\n",
      "# Load the trained model\n",
      "model = joblib.load('iris_model.pkl')\n",
      "\n",
      "# Create a FastAPI instance\n",
      "app = FastAPI()\n",
      "\n",
      "# Define the input data model\n",
      "class IrisInput(BaseModel):\n",
      "    sepal_length: float\n",
      "    sepal_width: float\n",
      "    petal_length: float\n",
      "    petal_width: float\n",
      "\n",
      "# Define the prediction endpoint\n",
      "@app.post(\"/predict\")\n",
      "def predict(iris_input: IrisInput):\n",
      "    # Prepare the input data for prediction\n",
      "    input_data = np.array([[iris_input.sepal_length, iris_input.sepal_width,\n",
      "                             iris_input.petal_length, iris_input.petal_width]])\n",
      "    \n",
      "    # Make prediction\n",
      "    try:\n",
      "        prediction = model.predict(input_data)\n",
      "        predicted_class = int(prediction[0])  # Get the predicted class as an integer\n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "    \n",
      "    # Return the prediction as JSON\n",
      "    return {\"predicted_class\": predicted_class}\n",
      "\n",
      "# Run the application\n",
      "if __name__ == \"__main__\":\n",
      "    import uvicorn\n",
      "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
      "```\n",
      "\n",
      "### Instructions to Run the Script\n",
      "\n",
      "1. **Install Required Packages**: Make sure you have FastAPI and Uvicorn installed. You can install them using pip:\n",
      "   ```bash\n",
      "   pip install fastapi uvicorn scikit-learn joblib\n",
      "   ```\n",
      "\n",
      "2. **Save the Script**: Save the above script in a file named `inference_api.py`.\n",
      "\n",
      "3. **Run the API**: Execute the script using the following command:\n",
      "   ```bash\n",
      "   python inference_api.py\n",
      "   ```\n",
      "\n",
      "4. **Access the API**: The API will be available at `http://127.0.0.1:8000/predict`. You can send a POST request with the input data in JSON format.\n",
      "\n",
      "### Example of Input JSON\n",
      "You can test the API using a tool like `curl` or Postman. Hereâ€™s an example of how to send a request using `curl`:\n",
      "\n",
      "```bash\n",
      "curl -X POST \"http://127.0.0.1:8000/predict\" -H \"Content-Type: application/json\" -d '{\"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2}'\n",
      "```\n",
      "\n",
      "### Expected Output\n",
      "The API will return a JSON response with the predicted class:\n",
      "```json\n",
      "{\n",
      "  \"predicted_class\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "This script is a complete and standalone FastAPI application that serves your trained model for inference.\n"
     ]
    }
   ],
   "source": [
    "print(raw_inference_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_python_script(llm_response):\n",
    "    \"\"\"\n",
    "    Extracts the main Python script from an LLM response.\n",
    "\n",
    "    Args:\n",
    "        llm_response (str): The response text containing a Python code block.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted Python script, or an empty string if no script is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```python\\n(.*?)\\n```\", llm_response, re.DOTALL)\n",
    "    return match.group(1) if match else \"\"\n",
    "\n",
    "\n",
    "inference_script = extract_python_script(raw_inference_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic import BaseModel\n",
      "import joblib\n",
      "import numpy as np\n",
      "\n",
      "# Load the trained model\n",
      "model = joblib.load('iris_model.pkl')\n",
      "\n",
      "# Create a FastAPI instance\n",
      "app = FastAPI()\n",
      "\n",
      "# Define the input data model\n",
      "class IrisInput(BaseModel):\n",
      "    sepal_length: float\n",
      "    sepal_width: float\n",
      "    petal_length: float\n",
      "    petal_width: float\n",
      "\n",
      "# Define the prediction endpoint\n",
      "@app.post(\"/predict\")\n",
      "def predict(iris_input: IrisInput):\n",
      "    # Prepare the input data for prediction\n",
      "    input_data = np.array([[iris_input.sepal_length, iris_input.sepal_width,\n",
      "                             iris_input.petal_length, iris_input.petal_width]])\n",
      "    \n",
      "    # Make prediction\n",
      "    try:\n",
      "        prediction = model.predict(input_data)\n",
      "        predicted_class = int(prediction[0])  # Get the predicted class as an integer\n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "    \n",
      "    # Return the prediction as JSON\n",
      "    return {\"predicted_class\": predicted_class}\n",
      "\n",
      "# Run the application\n",
      "if __name__ == \"__main__\":\n",
      "    import uvicorn\n",
      "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
     ]
    }
   ],
   "source": [
    "print(inference_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the inference script to a file\n",
    "inference_script_path = '/Users/neel/Developer/deploy_wizard/iris_model_inference/inference.py'\n",
    "write_to_file(inference_script_path, inference_script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_imported_libraries(python_script):\n",
    "    \"\"\"\n",
    "    Extracts imported libraries from a Python script.\n",
    "\n",
    "    Args:\n",
    "        python_script (str): The Python script content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique libraries found in the script.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r\"^\\s*(?:import|from)\\s+([\\w\\d_\\.]+)\", python_script, re.MULTILINE)\n",
    "    return list(set(matches))  # Remove duplicates\n",
    "\n",
    "\n",
    "def generate_requirements_with_llm(inference_script):\n",
    "    \"\"\"\n",
    "    Uses an LLM to generate a requirements.txt file based on the inference script.\n",
    "\n",
    "    Args:\n",
    "        inference_script (str): The Python inference script.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated requirements.txt content.\n",
    "    \"\"\"\n",
    "    imported_libraries = extract_imported_libraries(inference_script)\n",
    "    libraries_str = \", \".join(imported_libraries)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python package manager.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Given the following imported libraries: {libraries_str}\n",
    "\n",
    "        ### Task:\n",
    "        - Identify the necessary dependencies and versions.\n",
    "        - Generate a `requirements.txt` file.\n",
    "        - Ensure compatibility based on best practices.\n",
    "\n",
    "        ### Expected Output Format:\n",
    "        ```\n",
    "        package_name==X.Y.Z\n",
    "        package_name_2>=A.B.C\n",
    "        package_name_3\n",
    "        ```\n",
    "        - Each package should be on a new line.\n",
    "        - Include version constraints where necessary.\n",
    "        - Ensure that the packages are installable via pip.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    return requirements_generator_llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the libraries you've imported, here are the necessary dependencies and their recommended versions as of October 2023. I've ensured compatibility and best practices for each package.\n",
      "\n",
      "### Recommended Versions:\n",
      "- **pydantic**: The latest stable version is `1.10.9`. Pydantic is a data validation and settings management library that is widely used with FastAPI.\n",
      "- **fastapi**: The latest stable version is `0.95.2`. FastAPI is a modern web framework for building APIs with Python 3.6+ based on standard Python type hints.\n",
      "- **joblib**: The latest stable version is `1.3.2`. Joblib is used for lightweight pipelining in Python and is often used for saving and loading models.\n",
      "- **numpy**: The latest stable version is `1.24.3`. NumPy is a fundamental package for scientific computing with Python.\n",
      "- **uvicorn**: The latest stable version is `0.22.0`. Uvicorn is a lightning-fast ASGI server implementation, using `uvloop` and `httptools`.\n",
      "\n",
      "### Generated `requirements.txt`:\n",
      "```\n",
      "pydantic==1.10.9\n",
      "fastapi==0.95.2\n",
      "joblib==1.3.2\n",
      "numpy==1.24.3\n",
      "uvicorn==0.22.0\n",
      "```\n",
      "\n",
      "This `requirements.txt` file specifies exact versions for each package, ensuring that your environment is reproducible and compatible with the libraries you are using. You can install these dependencies using pip with the command:\n",
      "\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Create a new LLM instance for generating requirements.txt\n",
    "requirements_generator_llm = MyOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# Extract Python script from LLM response\n",
    "extracted_script = extract_python_script(raw_inference_script)\n",
    "\n",
    "# Generate the requirements.txt content\n",
    "requirements_txt = generate_requirements_with_llm(extracted_script)\n",
    "print(requirements_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requirements_txt(llm_response):\n",
    "    \"\"\"\n",
    "    Extracts the main contents of the `requirements.txt` file from an LLM response.\n",
    "\n",
    "    Args:\n",
    "        llm_response (str): The response text containing the `requirements.txt` section.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted `requirements.txt` content as a string.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```(?:\\w+\\n)?(.*?)\\n```\", llm_response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pydantic==1.10.9\n",
      "fastapi==0.95.2\n",
      "joblib==1.3.2\n",
      "numpy==1.24.3\n",
      "uvicorn==0.22.0\n"
     ]
    }
   ],
   "source": [
    "requirements_txt_content = extract_requirements_txt(requirements_txt)\n",
    "print(requirements_txt_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the requirements.txt to a file\n",
    "requirements_txt_path = '/Users/neel/Developer/deploy_wizard/iris_model_training/requirements.txt'\n",
    "write_to_file(requirements_txt_path, requirements_txt_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containerization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model_loading_with_llm(inference_script):\n",
    "    \"\"\"\n",
    "    Uses an LLM to modify the inference script, ensuring the model is loaded from an ENV variable if set,\n",
    "    while keeping the rest of the script unchanged.\n",
    "\n",
    "    Args:\n",
    "        inference_script (str): The raw inference script.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified inference script with the environment-based model loading.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python developer.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        The following Python script is an inference API.\n",
    "\n",
    "        ### Task:\n",
    "        - Identify the line(s) where the model is loaded (e.g., `model = joblib.load(\"model.pkl\")`).\n",
    "        - Modify only those lines to:\n",
    "          - Load the model from an environment variable `MODEL_PATH` if it is set.\n",
    "          - Otherwise, fall back to the original model path.\n",
    "        - Keep the rest of the script **unchanged**.\n",
    "\n",
    "        ### Expected Output:\n",
    "        The entire Python script with only the necessary modification applied.\n",
    "\n",
    "        ### Example Modification:\n",
    "        **Before:**\n",
    "        ```python\n",
    "        import joblib\n",
    "        model = joblib.load(\"model.pkl\")\n",
    "        ```\n",
    "\n",
    "        **After:**\n",
    "        ```python\n",
    "        import os\n",
    "        MODEL_PATH = os.getenv(\"MODEL_PATH\", \"model.pkl\")\n",
    "\n",
    "        import joblib\n",
    "        model = joblib.load(MODEL_PATH)\n",
    "        ```\n",
    "\n",
    "        Now, apply the same transformation to the given script.\n",
    "\n",
    "        ### Script:\n",
    "        ```\n",
    "        {inference_script}\n",
    "        ```\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    return miss_llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the modified Python script with the necessary changes applied to load the model from an environment variable `MODEL_PATH` if it is set, otherwise falling back to the original model path:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic import BaseModel\n",
      "import joblib\n",
      "import numpy as np\n",
      "import os\n",
      "\n",
      "# Load the trained model\n",
      "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"iris_model.pkl\")\n",
      "model = joblib.load(MODEL_PATH)\n",
      "\n",
      "# Create a FastAPI instance\n",
      "app = FastAPI()\n",
      "\n",
      "# Define the input data model\n",
      "class IrisInput(BaseModel):\n",
      "    sepal_length: float\n",
      "    sepal_width: float\n",
      "    petal_length: float\n",
      "    petal_width: float\n",
      "\n",
      "# Define the prediction endpoint\n",
      "@app.post(\"/predict\")\n",
      "def predict(iris_input: IrisInput):\n",
      "    # Prepare the input data for prediction\n",
      "    input_data = np.array([[iris_input.sepal_length, iris_input.sepal_width,\n",
      "                             iris_input.petal_length, iris_input.petal_width]])\n",
      "    \n",
      "    # Make prediction\n",
      "    try:\n",
      "        prediction = model.predict(input_data)\n",
      "        predicted_class = int(prediction[0])  # Get the predicted class as an integer\n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "    \n",
      "    # Return the prediction as JSON\n",
      "    return {\"predicted_class\": predicted_class}\n",
      "\n",
      "# Run the application\n",
      "if __name__ == \"__main__\":\n",
      "    import uvicorn\n",
      "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
      "``` \n",
      "\n",
      "In this modified script, the model is now loaded from the `MODEL_PATH` environment variable, with a fallback to `'iris_model.pkl'`.\n"
     ]
    }
   ],
   "source": [
    "miss_llm = MyOpenAI(model=MODEL_NAME)\n",
    "# Step 2: Modify the model loading logic through an LLM without changing other parts\n",
    "updated_inference_script = modify_model_loading_with_llm(inference_script)\n",
    "\n",
    "# Step 3: Save or use the modified script\n",
    "print(updated_inference_script)  # This script now correctly loads the model from ENV variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etract the updated inference script and save it\n",
    "updated_inference_script = extract_python_script(updated_inference_script)\n",
    "updated_inference_script_path = '/Users/neel/Developer/deploy_wizard/iris_model_inference/inference.py'\n",
    "write_to_file(updated_inference_script_path, updated_inference_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify all paths\n",
    "inference_script_path = '/Users/neel/Developer/deploy_wizard/iris_model_inference/inference.py'\n",
    "requirements_txt_path = '/Users/neel/Developer/deploy_wizard/iris_model_inference/requirements.txt'\n",
    "dockerfile_path = '/Users/neel/Developer/deploy_wizard/iris_model_inference/Dockerfile'\n",
    "model_path = '/Users/neel/Developer/deploy_wizard/iris_model_inference/iris_model.pkl'\n",
    "#read all files \n",
    "inference_script = file_reader(inference_script_path)\n",
    "requirements_txt = file_reader(requirements_txt_path)\n",
    "dockerfile = file_reader(dockerfile_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dockerfile(options, feedback=None):\n",
    "    if feedback is None:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a Docker expert.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'''\n",
    "        You are an expert Dockerfile Generation Agent. Your task is to generate a **Dockerfile** that encapsulates a complete model inference environment.\n",
    "\n",
    "        ### Role:\n",
    "        Assist in writing a Dockerfile for a containerized model inference application using **only relative paths**.\n",
    "\n",
    "        ### Context:\n",
    "        - **Working Directory**: The Dockerfile is located inside the project directory where the model inference script, requirements file, and model file are also stored.\n",
    "        - **Inference Script**:\n",
    "            - Location (relative to the Dockerfile): `{options['inference_script_path']}`\n",
    "            - Contents:\n",
    "            ```\n",
    "            {options['inference_script_content']}\n",
    "            ```\n",
    "        - **Requirements File**:\n",
    "            - Location (relative to the Dockerfile): `{options['requirements_txt_path']}`\n",
    "            - Contents:\n",
    "            ```\n",
    "            {options['requirements_txt_content']}\n",
    "            ```\n",
    "        - **Model File**:\n",
    "            - Location (relative to the Dockerfile): `{options['model_path']}`\n",
    "\n",
    "        ### Action:\n",
    "        Generate a **Dockerfile** that:\n",
    "        - Uses an appropriate base image (e.g., `python:3.9-slim`).\n",
    "        - Sets the working directory to `/app`.\n",
    "        - Installs Python dependencies from the provided requirements file.\n",
    "        - **Copies all necessary files using relative paths (i.e., no absolute paths)**.\n",
    "        - Ensures that the model file is placed inside `/app/model/` and creates the directory if needed.\n",
    "        - Configures any necessary environment variables (e.g., `MODEL_PATH=/app/model/{options['model_path'].split('/')[-1]}`).\n",
    "        - Exposes a port if the inference script serves an API (e.g., FastAPI exposing `8000`).\n",
    "        - Specifies the command or entrypoint to run the inference script.\n",
    "        - Follows **best practices** for caching and cleanup (e.g., using `--no-cache-dir` for `pip install`).\n",
    "\n",
    "        ### Additional Constraints:\n",
    "        - **Do not use absolute paths.**  \n",
    "        - **Assume all files are within the same directory as the Dockerfile when running `docker build .`.**\n",
    "        - **Ensure the COPY commands properly reflect this.**\n",
    "        \n",
    "        ### Expected Output:\n",
    "        Provide a complete **Dockerfile** as a code block with Dockerfile syntax highlighting.\n",
    "        '''\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        messages = feedback\n",
    "    response = docker_generator_llm.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_generator_llm = MyOpenAI(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_for_dockerfile = {\n",
    "    \"inference_script_path\": inference_script_path,\n",
    "    \"inference_script_content\": inference_script,\n",
    "    \"requirements_txt_path\": requirements_txt_path,\n",
    "    \"requirements_txt_content\": requirements_txt,\n",
    "    \"model_path\": \"/Users/neel/Developer/deploy_wizard/iris_model_inference/iris_model.pkl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dockerfile = generate_dockerfile(context_for_dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dockerfile(llm_response):\n",
    "    \"\"\"\n",
    "    Extracts the main Python script from an LLM response.\n",
    "\n",
    "    Args:\n",
    "        llm_response (str): The response text containing a Python code block.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted Dockerfile, or an empty string if no script is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```Dockerfile\\n(.*?)\\n```\", llm_response, re.DOTALL)\n",
    "    return match.group(1) if match else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Use an appropriate base image\n",
      "FROM python:3.9-slim\n",
      "\n",
      "# Set the working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the requirements file and install dependencies\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Create a directory for the model\n",
      "RUN mkdir -p model\n",
      "\n",
      "# Copy the model file and inference script into the container\n",
      "COPY iris_model.pkl model/\n",
      "COPY inference.py .\n",
      "\n",
      "# Set environment variable for the model path\n",
      "ENV MODEL_PATH=/app/model/iris_model.pkl\n",
      "\n",
      "# Expose the port for the FastAPI application\n",
      "EXPOSE 8000\n",
      "\n",
      "# Specify the command to run the inference script\n",
      "CMD [\"python\", \"inference.py\"]\n"
     ]
    }
   ],
   "source": [
    "dockerfile = extract_dockerfile(raw_dockerfile)\n",
    "print(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_path = '/Users/neel/Developer/deploy_wizard/iris_model_inference/Dockerfile'\n",
    "write_to_file(dockerfile_path, dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docker_test(messages):\n",
    "    response = docker_generator_llm.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the dockerized model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local testind and deployedment\n",
    "def dockerfile_testing():\n",
    "    print(\"We will now try validating that the Dockerfile is correctly set up for building and running the model inference application.\")\n",
    "    print(\"This will involve building the Docker image locally and running a container to test the model predictions.\")\n",
    "    print(\"Let's start by building the Docker image.\")\n",
    "    message =\"Can you provide me instructions on how to build this docker image and run it, locally?\"\n",
    "    while True:\n",
    "        response = docker_generator_llm.invoke(message)\n",
    "        print(response)\n",
    "        message = input(\"Enter your response, if everything is working say gg: \")\n",
    "        if message == 'gg':\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now try validating that the Dockerfile is correctly set up for building and running the model inference application.\n",
      "This will involve building the Docker image locally and running a container to test the model predictions.\n",
      "Let's start by building the Docker image.\n",
      "Certainly! Here are the step-by-step instructions to build and run the Docker image locally:\n",
      "\n",
      "### Prerequisites\n",
      "1. **Docker Installed**: Ensure you have Docker installed on your machine. You can download it from [Docker's official website](https://www.docker.com/get-started).\n",
      "\n",
      "### Instructions\n",
      "\n",
      "1. **Navigate to the Project Directory**:\n",
      "   Open a terminal and navigate to the directory where your `Dockerfile`, `requirements.txt`, `inference.py`, and `iris_model.pkl` files are located. For example:\n",
      "   ```bash\n",
      "   cd /Users/neel/Developer/deploy_wizard/iris_model_inference\n",
      "   ```\n",
      "\n",
      "2. **Build the Docker Image**:\n",
      "   Use the following command to build the Docker image. You can name the image (e.g., `iris_model_inference`) as you prefer:\n",
      "   ```bash\n",
      "   docker build -t iris_model_inference .\n",
      "   ```\n",
      "   The `-t` flag allows you to tag the image with a name. The `.` at the end specifies the current directory as the build context.\n",
      "\n",
      "3. **Run the Docker Container**:\n",
      "   After the image is built successfully, you can run a container from the image using the following command:\n",
      "   ```bash\n",
      "   docker run -d -p 8000:8000 --name iris_inference_container iris_model_inference\n",
      "   ```\n",
      "   - The `-d` flag runs the container in detached mode (in the background).\n",
      "   - The `-p 8000:8000` flag maps port 8000 of the container to port 8000 on your host machine.\n",
      "   - The `--name` flag gives a name to the running container (in this case, `iris_inference_container`).\n",
      "\n",
      "4. **Access the FastAPI Application**:\n",
      "   Once the container is running, you can access the FastAPI application by navigating to the following URL in your web browser or using a tool like `curl` or Postman:\n",
      "   ```\n",
      "   http://localhost:8000/docs\n",
      "   ```\n",
      "   This will take you to the automatically generated API documentation where you can test the `/predict` endpoint.\n",
      "\n",
      "5. **Stopping the Container**:\n",
      "   If you want to stop the running container, you can use the following command:\n",
      "   ```bash\n",
      "   docker stop iris_inference_container\n",
      "   ```\n",
      "\n",
      "6. **Removing the Container** (Optional):\n",
      "   If you want to remove the container after stopping it, you can use:\n",
      "   ```bash\n",
      "   docker rm iris_inference_container\n",
      "   ```\n",
      "\n",
      "### Summary\n",
      "- Build the image with `docker build -t iris_model_inference .`\n",
      "- Run the container with `docker run -d -p 8000:8000 --name iris_inference_container iris_model_inference`\n",
      "- Access the API at `http://localhost:8000/docs`\n",
      "\n",
      "That's it! You should now have your model inference application running in a Docker container.\n",
      "The error message you're encountering indicates that the `joblib` library is trying to load a model that was likely trained using `scikit-learn`, but the `scikit-learn` library is not installed in your Docker container. To resolve this issue, you need to add `scikit-learn` to your `requirements.txt` file.\n",
      "\n",
      "### Steps to Fix the Issue\n",
      "\n",
      "1. **Update `requirements.txt`**:\n",
      "   Open your `requirements.txt` file and add `scikit-learn` to the list of dependencies. Your updated `requirements.txt` should look like this:\n",
      "\n",
      "   ```\n",
      "   pydantic==1.10.9\n",
      "   fastapi==0.95.2\n",
      "   joblib==1.3.2\n",
      "   numpy==1.24.3\n",
      "   uvicorn==0.22.0\n",
      "   scikit-learn==1.1.3  # Add this line\n",
      "   ```\n",
      "\n",
      "   Make sure to specify the version of `scikit-learn` that is compatible with your model if you know it. If you're unsure, you can just use `scikit-learn` without a version number to get the latest version.\n",
      "\n",
      "2. **Rebuild the Docker Image**:\n",
      "   After updating the `requirements.txt`, you need to rebuild the Docker image to include the new dependency. Run the following command in your terminal:\n",
      "\n",
      "   ```bash\n",
      "   docker build -t iris_model_inference .\n",
      "   ```\n",
      "\n",
      "3. **Run the Docker Container Again**:\n",
      "   Once the image is rebuilt successfully, run the container again:\n",
      "\n",
      "   ```bash\n",
      "   docker run -d -p 8000:8000 --name iris_inference_container iris_model_inference\n",
      "   ```\n",
      "\n",
      "4. **Check Logs for Errors**:\n",
      "   If the container fails to start again, you can check the logs to see the error messages:\n",
      "\n",
      "   ```bash\n",
      "   docker logs iris_inference_container\n",
      "   ```\n",
      "\n",
      "### Summary\n",
      "- Add `scikit-learn` to your `requirements.txt`.\n",
      "- Rebuild the Docker image.\n",
      "- Run the container again.\n",
      "\n",
      "This should resolve the `ModuleNotFoundError` for `sklearn`, and your model inference application should work as expected. If you encounter any further issues, feel free to ask!\n",
      "The version of `scikit-learn` you should add to your `requirements.txt` depends on the version used when the model was trained. If you have access to the environment where the model was created, you can check the version of `scikit-learn` that was used. \n",
      "\n",
      "However, if you don't have that information, you can use a stable and commonly used version. As of my last knowledge update in October 2023, `scikit-learn` version `1.1.3` is a stable release that should work for most use cases. \n",
      "\n",
      "Hereâ€™s how you can update your `requirements.txt`:\n",
      "\n",
      "```plaintext\n",
      "pydantic==1.10.9\n",
      "fastapi==0.95.2\n",
      "joblib==1.3.2\n",
      "numpy==1.24.3\n",
      "uvicorn==0.22.0\n",
      "scikit-learn==1.1.3  # Recommended stable version\n",
      "```\n",
      "\n",
      "### Steps to Check the Version\n",
      "If you want to ensure compatibility with your model, you can check the version of `scikit-learn` used in your original environment by running the following command in that environment:\n",
      "\n",
      "```bash\n",
      "pip show scikit-learn\n",
      "```\n",
      "\n",
      "This will display the version of `scikit-learn` installed. You can then use that version in your `requirements.txt`.\n",
      "\n",
      "### Final Note\n",
      "If you are unsure about the version and want to use the latest stable version, you can simply specify `scikit-learn` without a version number:\n",
      "\n",
      "```plaintext\n",
      "scikit-learn\n",
      "```\n",
      "\n",
      "This will install the latest version available at the time of building the Docker image. However, be cautious as this may lead to compatibility issues if there are breaking changes in future releases.\n"
     ]
    }
   ],
   "source": [
    "dockerfile_testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push to dockerhub (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are the step-by-step instructions for pushing your Docker container to Docker Hub:\n",
      "\n",
      "### Prerequisites\n",
      "1. **Docker Installed**: Ensure you have Docker installed on your machine.\n",
      "2. **Docker Hub Account**: You need to have a Docker Hub account. If you don't have one, you can sign up at [Docker Hub](https://hub.docker.com/).\n",
      "\n",
      "### Instructions\n",
      "\n",
      "1. **Log in to Docker Hub**:\n",
      "   Open a terminal and log in to your Docker Hub account using the following command:\n",
      "   ```bash\n",
      "   docker login\n",
      "   ```\n",
      "   You will be prompted to enter your Docker Hub username and password.\n",
      "\n",
      "2. **Tag Your Docker Image**:\n",
      "   Before pushing your image, you need to tag it with your Docker Hub username and the repository name. The format is:\n",
      "   ```\n",
      "   <username>/<repository>:<tag>\n",
      "   ```\n",
      "   For example, if your Docker Hub username is `yourusername` and you want to name your repository `iris_model_inference`, you can tag your image like this:\n",
      "   ```bash\n",
      "   docker tag iris_model_inference yourusername/iris_model_inference:latest\n",
      "   ```\n",
      "   Replace `yourusername` with your actual Docker Hub username.\n",
      "\n",
      "3. **Push the Docker Image**:\n",
      "   Now you can push the tagged image to Docker Hub using the following command:\n",
      "   ```bash\n",
      "   docker push yourusername/iris_model_inference:latest\n",
      "   ```\n",
      "   This command will upload your image to the specified repository on Docker Hub.\n",
      "\n",
      "4. **Verify the Push**:\n",
      "   After the push is complete, you can verify that your image is available on Docker Hub by visiting your Docker Hub profile at [Docker Hub](https://hub.docker.com/) and checking the repositories.\n",
      "\n",
      "### Summary\n",
      "- Log in to Docker Hub with `docker login`.\n",
      "- Tag your image with `docker tag iris_model_inference yourusername/iris_model_inference:latest`.\n",
      "- Push the image with `docker push yourusername/iris_model_inference:latest`.\n",
      "\n",
      "### Additional Notes\n",
      "- If you want to push a different version of your image, you can change the `<tag>` part in the tagging command (e.g., `:v1.0`, `:latest`, etc.).\n",
      "- Make sure your image is built and available locally before attempting to push it to Docker Hub.\n",
      "- If you encounter any issues during the push, check your internet connection and ensure that you are logged in to Docker Hub.\n"
     ]
    }
   ],
   "source": [
    "msg = \"Can You provide me instruction for pushing the container to dockerhub\"\n",
    "response = docker_generator_llm.invoke(msg)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## terraform for EC2 instance creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "terraform_generator_llm = MyOpenAI(model=MODEL_NAME)\n",
    "terraform_validator_llm = MyOpenAI(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_main_tf = file_reader('/Users/neel/Developer/deploy_wizard/templates/aws_ec2/main.tf')\n",
    "base_vars_tf = file_reader('/Users/neel/Developer/deploy_wizard/templates/aws_ec2/variables.tf')\n",
    "# Define parameters in a dictionary\n",
    "params = {\n",
    "    \"vpc_id\": \"vpc-0f0aea174086b6625\",\n",
    "    \"region\": \"us-west-1\",\n",
    "    \"subnet_id\": \"subnet-070d54662e68443ed\",\n",
    "    \"instance_type\": \"t2.micro\",\n",
    "    \"security_group_id\": \"sg-03aa3023dd84cf4a5\",\n",
    "    \"key_pair_name\": \"neel_test\",\n",
    "    \"ami_id\": \"ami-08d4f6bbae664bd41\",\n",
    "    \"model_port\": 8000,\n",
    "    \"model_name\": \"iris_model\",\n",
    "    \"image_name\": \"neel26d/iris_model_inference:latest\",\n",
    "    \"container_name\": \"iris_model\",\n",
    "    \"main_tf\": base_main_tf,\n",
    "    \"vars_tf\": base_vars_tf\n",
    "}\n",
    "def generate_ec2_terraform_file(params,feedback=None):\n",
    "    if not feedback:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Terraform developer.\"},\n",
    "            {\"role\": \"user\",\n",
    "            \"content\": f'''\n",
    "            ## Role:\n",
    "            You are an **expert DevOps assistant** specializing in Terraform and AWS EC2 deployments. Your task is to generate valid Terraform files (`main.tf` and `variables.tf`) while maintaining correct syntax.\n",
    "\n",
    "            ## Problem Statement:\n",
    "            A user wants to deploy a **machine learning model** as an API on an **AWS EC2 instance using Docker**. The Terraform script should:\n",
    "            - Create an **EC2 instance** inside a specified **VPC and Subnet**.\n",
    "            - Configure a **Security Group** to allow **SSH, HTTP, HTTPS, and API traffic**.\n",
    "            - Pull a Docker image and run it on the EC2 instance.\n",
    "            - Replace placeholders with provided variable values.\n",
    "            - For rules in securtiy group if no ips are specified then consider all ips 0.0.0.0/0\n",
    "\n",
    "            ## **Terraform Templates**\n",
    "            ### `main.tf`\n",
    "            {params['main_tf']} \n",
    "            +/n/n\n",
    "            ### `variables.tf`\n",
    "            {params['vars_tf']} \n",
    "\n",
    "            ##Provided values:\n",
    "            - **VPC ID**: {params['vpc_id']}\n",
    "            - **Region**: {params['region']}\n",
    "            - **Subnet ID**: {params['subnet_id']}\n",
    "            - **Instance Type**: {params['instance_type']}\n",
    "            - **Security Group ID**: {params['security_group_id']}\n",
    "            - **Key Pair Name**: {params['key_pair_name']}\n",
    "            - **AMI ID**: {params['ami_id']}\n",
    "            - **Model Port**: {params['model_port']}\n",
    "            - **Model Name**: {params['model_name']}   \n",
    "            - **Image Name**: {params['image_name']}\n",
    "            - **Container Name**: {params['container_name']}\n",
    "            - Make sure these are set as default values in values.yaml\n",
    "\n",
    "            ## Expected Output:\n",
    "            - **Replace all placeholders** in `main.tf` and `variables.tf` with actual values.\n",
    "            - **Maintain correct Terraform syntax**.\n",
    "            - **Return two separate files (`main.tf` and `variables.tf`)**.\n",
    "            - **Ensure the Terraform script is valid and deployable**.\n",
    "            - **Output must be structured** as separate code blocks for `main.tf` and `variables.tf`.\n",
    "            - It should be ``hcl \\n ### filename`` syntax highlighted.\n",
    "            '''}\n",
    "        ]\n",
    "    else:\n",
    "        messages = feedback\n",
    "    response = terraform_generator_llm.invoke(messages)\n",
    "    return response\n",
    "\n",
    "def evaluate_terraform_with_llm(terraform_code):\n",
    "    \"\"\"Use an LLM to evaluate the generated Terraform script and provide feedback.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a Terraform expert and code reviewer specializing in AWS infrastructure.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        You are reviewing a Terraform script that provisions an AWS EC2 instance.\n",
    "\n",
    "        ### Task:\n",
    "        - Analyze the script for correctness, best practices, and security compliance.\n",
    "        - Identify any missing components or improvements.\n",
    "        - If the script is perfect, respond with 'No changes needed.'\n",
    "        - If changes are required, specify what needs to be improved.\n",
    "        - Make Sure all variables are set as default values in values.yaml based on the values given\n",
    "        - If some value is explicitly mentioned in the script then it should be set as default value in values.yaml\n",
    "        - Dont use any new variables in the script, or use values not provided in the prompt\n",
    "\n",
    "\n",
    "        ### Script to review:\n",
    "        ```\n",
    "        {terraform_code}\n",
    "        ```\n",
    "\n",
    "        ### Expected Output:\n",
    "        - Either 'No changes needed.' OR a detailed improvement plan.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    return terraform_validator_llm.invoke(messages)\n",
    "\n",
    "\n",
    "def actor_critic_terraform(params):\n",
    "    \"\"\"\n",
    "    Runs the actor-critic loop for Terraform:\n",
    "    - Actor generates Terraform code.\n",
    "    - Critic evaluates the script.\n",
    "    - If the script is sufficient, exit early.\n",
    "    - Otherwise, Actor refines the script based on feedback.\n",
    "    - Max iterations: 2\n",
    "    \"\"\"\n",
    "    feedback = None  # No feedback initially\n",
    "    # Step 1: Actor generates Terraform script\n",
    "    terraform_script = generate_ec2_terraform_file(params, feedback)\n",
    "    for _ in range(2):  # Max iterations: 2\n",
    "        # Step 2: Critic evaluates the Terraform script\n",
    "        critic_feedback = evaluate_terraform_with_llm(terraform_script)\n",
    "\n",
    "        if \"no changes needed\" in critic_feedback.lower():\n",
    "            print(\"âœ… Terraform script is satisfactory. Exiting early.\")\n",
    "            return terraform_script  # Early exit if script is sufficient\n",
    "\n",
    "        # Step 3: Actor refines Terraform script using critic's feedback\n",
    "        feedback = [\n",
    "            {\"role\": \"assistant\", \"content\": terraform_script},\n",
    "            {\"role\": \"user\", \"content\": f\"Revise based on this feedback:\\n{critic_feedback}\"}\n",
    "        ]\n",
    "\n",
    "        terraform_script = generate_ec2_terraform_file(params, feedback)\n",
    "\n",
    "\n",
    "    print(\"ðŸ”„ Max iterations reached. Returning final Terraform script.\")\n",
    "    return terraform_script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Max iterations reached. Returning final Terraform script.\n"
     ]
    }
   ],
   "source": [
    "raw_terraform_script = actor_critic_terraform(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hereâ€™s a revised version of the Terraform script and `values.yaml` file based on your feedback. The changes focus on improving security, clarity, and usability.\n",
      "\n",
      "### Revised `main.tf`\n",
      "```hcl\n",
      "provider \"aws\" {\n",
      "  region = var.aws_region\n",
      "}\n",
      "\n",
      "resource \"aws_security_group\" \"ec2_sg\" {\n",
      "  name_prefix = \"${var.resource_prefix}-sg-\"\n",
      "\n",
      "  # Allow SSH access from a specific IP address\n",
      "  ingress {\n",
      "    from_port   = 22\n",
      "    to_port     = 22\n",
      "    protocol    = \"tcp\"\n",
      "    cidr_blocks = [var.ssh_cidr_block] # Ensure this is set to a specific IP\n",
      "  }\n",
      "\n",
      "  # Allow HTTP access (consider restricting this as needed)\n",
      "  ingress {\n",
      "    from_port   = 80\n",
      "    to_port     = 80\n",
      "    protocol    = \"tcp\"\n",
      "    cidr_blocks = [var.http_cidr_block] # Restrict this as needed\n",
      "  }\n",
      "\n",
      "  # Allow HTTPS access (consider restricting this as needed)\n",
      "  ingress {\n",
      "    from_port   = 443\n",
      "    to_port     = 443\n",
      "    protocol    = \"tcp\"\n",
      "    cidr_blocks = [var.https_cidr_block] # Restrict this as needed\n",
      "  }\n",
      "\n",
      "  # Allow requests to come to the model\n",
      "  ingress {\n",
      "    from_port   = 8000\n",
      "    to_port     = 8000\n",
      "    protocol    = \"tcp\"\n",
      "    cidr_blocks = [var.model_cidr_block] # Restrict this as needed\n",
      "  }\n",
      "\n",
      "  # Allow all outbound traffic\n",
      "  egress {\n",
      "    from_port   = 0\n",
      "    to_port     = 0\n",
      "    protocol    = \"-1\"\n",
      "    cidr_blocks = [\"0.0.0.0/0\"]\n",
      "  }\n",
      "\n",
      "  tags = {\n",
      "    Name = \"${var.resource_prefix}-ec2-sg\"\n",
      "  }\n",
      "}\n",
      "\n",
      "resource \"aws_instance\" \"model_server\" {\n",
      "  ami                    = var.ami_id\n",
      "  instance_type         = var.instance_type\n",
      "  subnet_id             = var.subnet_id\n",
      "  vpc_security_group_ids = [aws_security_group.ec2_sg.id]\n",
      "  key_name              = var.key_name\n",
      "\n",
      "  user_data = <<EOF\n",
      "#!/bin/bash\n",
      "set -e\n",
      "\n",
      "# Update and install Docker\n",
      "if ! command -v docker &> /dev/null; then\n",
      "  sudo yum update -y\n",
      "  sudo yum install -y docker\n",
      "  sudo systemctl start docker\n",
      "  sudo systemctl enable docker\n",
      "  sudo usermod -aG docker ec2-user\n",
      "fi\n",
      "\n",
      "# Pull the latest Docker image\n",
      "if ! sudo docker pull ${var.docker_image}; then\n",
      "  echo \"Failed to pull Docker image\" | tee -a /var/log/user-data.log\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "# Run the container\n",
      "if ! sudo docker run -d -p 8000:8000 --name ${var.container_name} ${var.docker_image}; then\n",
      "  echo \"Failed to run Docker container\" | tee -a /var/log/user-data.log\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "echo \"Model server started successfully\" | tee -a /var/log/user-data.log\n",
      "EOF\n",
      "\n",
      "  tags = {\n",
      "    Name = \"${var.resource_prefix}-model-server\"\n",
      "  }\n",
      "}\n",
      "\n",
      "output \"model_server_public_ip\" {\n",
      "  value = aws_instance.model_server.public_ip\n",
      "}\n",
      "\n",
      "output \"model_server_public_dns\" {\n",
      "  value = aws_instance.model_server.public_dns\n",
      "}\n",
      "```\n",
      "\n",
      "### Revised `variables.tf`\n",
      "```hcl\n",
      "variable \"aws_region\" {\n",
      "  description = \"AWS region\"\n",
      "  type        = string\n",
      "  default     = \"us-west-1\"\n",
      "}\n",
      "\n",
      "variable \"ami_id\" {\n",
      "  description = \"AMI ID for the EC2 instance\"\n",
      "  type        = string\n",
      "  default     = \"ami-08d4f6bbae664bd41\"\n",
      "}\n",
      "\n",
      "variable \"instance_type\" {\n",
      "  description = \"EC2 instance type\"\n",
      "  type        = string\n",
      "  default     = \"t2.micro\" # Consider evaluating instance type for production workloads\n",
      "}\n",
      "\n",
      "variable \"subnet_id\" {\n",
      "  description = \"Subnet ID where the instance will be launched\"\n",
      "  type        = string\n",
      "  default     = \"subnet-070d54662e68443ed\"\n",
      "}\n",
      "\n",
      "variable \"key_name\" {\n",
      "  description = \"SSH key pair name for accessing the instance\"\n",
      "  type        = string\n",
      "  default     = \"neel_test\"\n",
      "}\n",
      "\n",
      "variable \"docker_image\" {\n",
      "  description = \"Docker image for model deployment\"\n",
      "  type        = string\n",
      "  default     = \"neel26d/iris_model_inference:latest\"\n",
      "}\n",
      "\n",
      "variable \"container_name\" {\n",
      "  description = \"Name of the Docker container\"\n",
      "  type        = string\n",
      "  default     = \"iris_model\"\n",
      "}\n",
      "\n",
      "variable \"resource_prefix\" {\n",
      "  description = \"Prefix for resource naming\"\n",
      "  type        = string\n",
      "  default     = \"iris\"\n",
      "}\n",
      "\n",
      "variable \"ssh_cidr_block\" {\n",
      "  description = \"CIDR block for SSH access\"\n",
      "  type        = string\n",
      "  default     = \"YOUR_IP_ADDRESS/32\" # Replace with your actual IP address\n",
      "}\n",
      "\n",
      "variable \"http_cidr_block\" {\n",
      "  description = \"CIDR block for HTTP access\"\n",
      "  type        = string\n",
      "  default     = \"0.0.0.0/0\" # Change as needed\n",
      "}\n",
      "\n",
      "variable \"https_cidr_block\" {\n",
      "  description = \"CIDR block for HTTPS access\"\n",
      "  type        = string\n",
      "  default     = \"0.0.0.0/0\" # Change as needed\n",
      "}\n",
      "\n",
      "variable \"model_cidr_block\" {\n",
      "  description = \"CIDR block for model access\"\n",
      "  type        = string\n",
      "  default     = \"0.0.0.0/0\" # Change as needed\n",
      "}\n",
      "```\n",
      "\n",
      "### Revised `values.yaml`\n",
      "```yaml\n",
      "aws_region: \"us-west-1\"\n",
      "ami_id: \"ami-08d4f6bbae664bd41\"\n",
      "instance_type: \"t2.micro\" # Consider evaluating instance type for production workloads\n",
      "subnet_id: \"subnet-070d54662e68443ed\"\n",
      "key_name: \"neel_test\"\n",
      "docker_image: \"neel26d/iris_model_inference:latest\"\n",
      "container_name: \"iris_model\"\n",
      "resource_prefix: \"iris\"\n",
      "ssh_cidr_block: \"YOUR_IP_ADDRESS/32\" # Replace with your actual IP address\n",
      "http_cidr_block: \"0.0.0.0/0\" # Change as needed\n",
      "https_cidr_block: \"0.0.0.0/0\" # Change as needed\n",
      "model_cidr_block: \"0.0.0.0/0\" # Change as needed\n",
      "```\n",
      "\n",
      "### Summary of Changes\n",
      "1. **Security Group CIDR Blocks**: The `ssh_cidr_block` retains a placeholder but is clearly marked for user replacement.\n",
      "2. **User Data Script Logging**: Added logging to the user data script to help with debugging if the instance fails to start correctly.\n",
      "3. **Resource Naming**: The security group name is prefixed with a more descriptive name to avoid conflicts.\n",
      "4. **Instance Type Note**: Added a comment in the `variables.tf` and `values.yaml` to encourage users to evaluate their instance type for production workloads.\n",
      "5. **Output Clarity**: The output names remain consistent, but the naming convention is clear and descriptive.\n",
      "\n",
      "These changes enhance the security, usability, and maintainability of the Terraform configuration while addressing the feedback provided.\n"
     ]
    }
   ],
   "source": [
    "print(raw_terraform_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extract_and_save_hcl_files(response_str, folder=\"aws_terraform_files\"):\n",
    "    \"\"\"\n",
    "    Extracts HCL code blocks from the given string and saves each block\n",
    "    to one or more files in the specified folder.\n",
    "\n",
    "    :param response_str: The string containing HCL code blocks.\n",
    "    :param folder: The folder where the files will be saved (default is \"aws_terraform_files\").\n",
    "    \"\"\"\n",
    "    # Ensure the target folder exists.\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    # Regular expression to capture all HCL code blocks\n",
    "    pattern = r\"```hcl\\s*(.*?)```\"\n",
    "    blocks = re.findall(pattern, response_str, re.DOTALL)\n",
    "\n",
    "    for block in blocks:\n",
    "        lines = block.strip().splitlines()\n",
    "        current_filename = None\n",
    "        current_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "\n",
    "            # Check if the line is a header line indicating a file name\n",
    "            if stripped_line.startswith(\"###\"):  # Detect lines like \"### main.tf\"\n",
    "                # Save the previous file before switching\n",
    "                if current_filename is not None and current_lines:\n",
    "                    \n",
    "                    file_path = os.path.join(folder, current_filename)\n",
    "                    with open(file_path, \"w\") as f:\n",
    "                        f.write(\"\\n\".join(current_lines).rstrip())\n",
    "                    print(f\"Wrote file: {file_path}\")\n",
    "\n",
    "                # Extract the file name\n",
    "                current_filename = stripped_line.split(\" \")[1].strip()\n",
    "                current_lines = []\n",
    "            else:\n",
    "                # If no header has been encountered yet, infer a filename\n",
    "                if current_filename is None:\n",
    "                    if 'resource \"aws_instance\"' in line:\n",
    "                        current_filename = \"main.tf\"\n",
    "                    elif 'variable' in line:\n",
    "                        current_filename = \"variables.tf\"\n",
    "                    elif 'output' in line:\n",
    "                        current_filename = \"outputs.tf\"\n",
    "                    else:\n",
    "                        current_filename = \"misc.tf\"  # Fallback default\n",
    "                current_lines.append(line)\n",
    "\n",
    "        # After processing all lines in the block, write out the last file\n",
    "        if current_filename and current_lines:\n",
    "            file_path = os.path.join(folder, current_filename)\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(current_lines).rstrip())\n",
    "            print(f\"Wrote file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file: aws_terraform_files/misc.tf\n",
      "Wrote file: aws_terraform_files/variables.tf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_and_save_hcl_files(raw_terraform_script)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below are the steps you need to take before running the Terraform script, along with instructions on how to execute it.\n",
      "\n",
      "### Pre-Execution Changes\n",
      "\n",
      "1. **Update `ssh_cidr_block`**:\n",
      "   - In the `values.yaml` file, replace the placeholder `YOUR_IP_ADDRESS/32` with your actual public IP address. This is crucial for allowing SSH access to the EC2 instance.\n",
      "   ```yaml\n",
      "   ssh_cidr_block: \"YOUR_ACTUAL_IP_ADDRESS/32\" # Replace with your actual IP address\n",
      "   ```\n",
      "\n",
      "2. **Review and Update Other CIDR Blocks**:\n",
      "   - If you want to restrict access for HTTP, HTTPS, or model access, update the `http_cidr_block`, `https_cidr_block`, and `model_cidr_block` in the `values.yaml` file as needed.\n",
      "\n",
      "3. **Check the AMI ID**:\n",
      "   - Ensure that the `ami_id` in the `values.yaml` file is valid for your selected AWS region. You can find the appropriate AMI ID in the AWS Management Console.\n",
      "\n",
      "4. **Select the Instance Type**:\n",
      "   - Review the `instance_type` in the `values.yaml` file. The default is set to `t2.micro`, which is suitable for testing but may not be ideal for production workloads. Change it if necessary.\n",
      "\n",
      "5. **Verify Subnet ID**:\n",
      "   - Ensure that the `subnet_id` in the `values.yaml` file corresponds to a valid subnet in your VPC.\n",
      "\n",
      "6. **SSH Key Pair**:\n",
      "   - Make sure the `key_name` in the `values.yaml` file matches an existing SSH key pair in your AWS account. If you need to create a new key pair, do so in the AWS Management Console.\n",
      "\n",
      "7. **Docker Image**:\n",
      "   - Verify that the `docker_image` in the `values.yaml` file points to a valid Docker image that you want to deploy.\n",
      "\n",
      "### Instructions for Running the Terraform Script\n",
      "\n",
      "1. **Install Terraform**:\n",
      "   - Ensure that you have Terraform installed on your local machine. You can download it from the [Terraform website](https://www.terraform.io/downloads.html).\n",
      "\n",
      "2. **Set Up AWS Credentials**:\n",
      "   - Make sure your AWS credentials are configured. You can do this by setting environment variables or using the AWS CLI. For example:\n",
      "   ```bash\n",
      "   export AWS_ACCESS_KEY_ID=\"your_access_key\"\n",
      "   export AWS_SECRET_ACCESS_KEY=\"your_secret_key\"\n",
      "   ```\n",
      "\n",
      "3. **Navigate to the Terraform Directory**:\n",
      "   - Open a terminal and navigate to the directory where your Terraform files (`main.tf`, `variables.tf`, `values.yaml`) are located.\n",
      "\n",
      "4. **Initialize Terraform**:\n",
      "   - Run the following command to initialize Terraform. This command downloads the necessary provider plugins.\n",
      "   ```bash\n",
      "   terraform init\n",
      "   ```\n",
      "\n",
      "5. **Validate the Configuration**:\n",
      "   - Itâ€™s a good practice to validate your Terraform configuration before applying it. Run:\n",
      "   ```bash\n",
      "   terraform validate\n",
      "   ```\n",
      "\n",
      "6. **Plan the Deployment**:\n",
      "   - Generate an execution plan to see what resources will be created. This step is useful for reviewing changes before applying them.\n",
      "   ```bash\n",
      "   terraform plan -var-file=\"values.yaml\"\n",
      "   ```\n",
      "\n",
      "7. **Apply the Configuration**:\n",
      "   - If everything looks good, apply the configuration to create the resources in AWS:\n",
      "   ```bash\n",
      "   terraform apply -var-file=\"values.yaml\"\n",
      "   ```\n",
      "   - You will be prompted to confirm the action. Type `yes` to proceed.\n",
      "\n",
      "8. **Access the EC2 Instance**:\n",
      "   - Once the resources are created, you can access the EC2 instance using SSH:\n",
      "   ```bash\n",
      "   ssh -i /path/to/your/private-key.pem ec2-user@<model_server_public_ip>\n",
      "   ```\n",
      "   - Replace `/path/to/your/private-key.pem` with the path to your SSH key and `<model_server_public_ip>` with the public IP address output by Terraform.\n",
      "\n",
      "9. **Monitor Logs**:\n",
      "   - If you encounter issues, you can check the logs on the EC2 instance by connecting via SSH and viewing the `/var/log/user-data.log` file for any errors during the startup process.\n",
      "\n",
      "### Cleanup\n",
      "\n",
      "When you are done with the resources and want to avoid incurring charges, you can destroy the resources created by Terraform with the following command:\n",
      "```bash\n",
      "terraform destroy -var-file=\"values.yaml\"\n",
      "```\n",
      "You will be prompted to confirm the destruction. Type `yes` to proceed.\n",
      "\n",
      "By following these steps, you should be able to successfully run the Terraform script and deploy your resources in AWS.\n"
     ]
    }
   ],
   "source": [
    "msg = \"Can you provde the things I will have to change before running the terraform script and instructuion on running it\"\n",
    "response = terraform_generator_llm.invoke(msg)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have a terraform code, i need to ensure it can do the following - I will provide\n",
    "# security group provides access to protocols and ports for all ip in the followinf format (port, protocol)\n",
    "# appropriate values in the terraform are present - i will\n",
    "# init script is present to install docker and other dependencies and run the docker image as I specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terraform_deployment_validator():\n",
    "    msg = \"Can you provide me instructions on how to deploy this terraform script?\"\n",
    "    response = terraform_generator_llm.invoke(msg)\n",
    "    print(response)\n",
    "    while True:\n",
    "        msg = input(\"Enter your response, if everything is working say gg: \")\n",
    "        if msg == 'gg':\n",
    "            break\n",
    "        response = terraform_generator_llm.invoke(msg)\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security group setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
